{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "638ab375",
   "metadata": {},
   "source": [
    "# OpenAICookbookPromptEngineering\n",
    "## Overview\n",
    "This notebook \n",
    "1. Scrapes java code from the fineract directory\n",
    "2. Cleans it up & tokenizes it\n",
    "3. Chunks the codebase into a max token size (defaults to 1600)\n",
    "4. Embeds the tokenized code chunks via openAI's `text-embedding-ada-002` model\n",
    "5. Stores the embedded text as a dataframe\n",
    "6. Uses the dataframe as extra context to provide to chatGPT based on the question asked of ChatGPT\n",
    "\n",
    "## Operations\n",
    "Assuming you're starting from scratch and opening this notebook for the first time, its cells offer the following functionalities (the numbers do not necessarily correspond to individual cells):\n",
    "\n",
    "1. The first cell intends to `pip install` all required python modules/libraries to make things slightly easier -- run a single cell and know that all dependencies are ensured. It doesn't really hurt anything to including another `import <package>` line in following cells though, so not a hard/fast rule.\n",
    "\n",
    "2. The second cell sets up your openAI API key against a local `openai-key.txt` file. Again, .gitignore includes that txt file, so as long as you name that file exactly when making your API key we should be alright. For this reason though, be sure to ALWAYS run `git status` before running any `git commit -am \"\"` command to ensure you do not accidentally include sensitive information.\n",
    "\n",
    "> **Warning**\n",
    "> Once any sensitive information is baked into ANY git commit & pushed to the remote repo, it exists within the git log. It is *possible* to remove it without destroying all code history, but it is very painstaking and sometimes difficult to detect after the fact. Always ensure that your commit history does not include any sensitive information prior to pushing to a remote repository.\n",
    "\n",
    "3. Git clones the fineract web app from the open-source repo on the 1.8.4 git branch. By default this bit of code is commented-out as it takes a while and the way it's written it will always attempt the git clone, so it's recommended to re-comment it after you've downloaded it locally. This repo's .gitignore knows to ignore this repository, so don't worry about nested git repositories.\n",
    "\n",
    "4. Sets up java cleanup function(s) to remove erraneous information that isn't important (comments, empty return character lines, & unit test code blocks). The removal of unit test code blocks is essential because including the word `test` in your prompt/question to ChatGPT may skew the context if there are too many text vectors that include the word `test` in them (which unit test code blocks typically do)\n",
    "\n",
    "5. Tokenizes the files & chunks them based on the chunksize provided (defaults to 1600). It uses the SentencePiece Byte-Pair Encoding Tokenizer from the `tokenizers` python module due to more correct formatting of the results. Other options are available (character, byte-level, or Bert Word) but the results weren't as great; these options are included within the import cell if experimentation is desired. The tokenized chunks are optionally written to a local flat directory `java_files_chunks_sentence` (two lines are marked as \"uncomment\" if you want to see the results written locally).\n",
    "\n",
    "6. The tokenized chunks are stored as a dataframe (and then ultimately a CSV file in the `data` directory) with vectorized strings. We define functions that provide that dataframe based on a query, with the query representing the question we'll use later on to ask ChatGPT a question about the fineract codebase (in our case as an attempt to write unit tests).\n",
    "\n",
    "7. We define our prompt & test the vector serach to make sure the files we're finding match the question we're asking\n",
    "\n",
    "8. We define the functions for how to ask ChatGPT a question & then use all the previous functions to search for extra context & provide that all over an API call\n",
    "\n",
    "\n",
    "## Sources\n",
    "This notebook stems from two OpenAI Coookbook templates:\n",
    "\n",
    "* [Embedding Wikipedia Tokens (Prerequisite for Next Example)](https://github.com/openai/openai-cookbook/blob/2a2753e8d0566fbf21a8270ce6afaf761d7cdee5/examples/Embedding_Wikipedia_articles_for_search.ipynb)\n",
    "* [Question Answering Using Embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n",
    "\n",
    "The OpenAICookbookPromptEngineering Notebook uses the apache fineract java web app [(git branch 1.8.4)](https://github.com/apache/fineract/tree/1.8.4) as its application code to consume from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e0ea3-bd06-4a03-b4a1-88a3f0bb84b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setup installation of packages and what to import/consume later:\n",
    "%pip install scipy\n",
    "%pip install tokenizers\n",
    "%pip install openai\n",
    "\n",
    "import regex\n",
    "import IPython.display\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import openai\n",
    "import re\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import CharBPETokenizer as CBPET\n",
    "from tokenizers import ByteLevelBPETokenizer as BBPET\n",
    "from tokenizers import SentencePieceBPETokenizer as SPBPET\n",
    "from tokenizers import BertWordPieceTokenizer as BWPT\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial  # for calculating vector similarities for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1cd74-bb1d-4cc6-b9c2-06bf76a8d768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the opeanAI API key:\n",
    "openai.api_key_path = 'openai-key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7021dc5-ecd1-426c-973c-03165085c5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Unit Test Scraper Functions\n",
    "Below Unit Test Finder functions find SOME unit tests in *.java files. Makes the following assumptions:\n",
    "    - Unit Test starts with \"@Test <return character><tab character> public void\"\n",
    "    - Unit Test does NOT throw any exception, and thus just has a return character after its declaration\n",
    "\n",
    "Also cleans up\n",
    "    - single-line comments\n",
    "    - multi-line comments\n",
    "    - empty new lines\n",
    "\n",
    "With the above assumptions it finds 465 of the 588 unit test cases littered throughout the fineract application\n",
    "\"\"\"\n",
    "\n",
    "# Regex Pattern to Scrap Unit Tests:\n",
    "singleline_pattern = r\"(?!\\/\\/localhost\\b)\\/\\/.*\"\n",
    "multiline_pattern = r\"/\\*(.|[\\r\\n])*?\\*/\"\n",
    "whiteline_pattern = r\"\\n\\s*\\n\"\n",
    "test_pattern = r\"@Test\\s*public\\s*void\\s*\\w+\\s*\\(\\s*\\)\\s*\\{[^{}]*+(?:(?:\\{[^{}]*+\\})*+[^{}]*+)*+\\}\"\n",
    "\n",
    "def remove_comments(java_code):\n",
    "    # Remove single-line comments\n",
    "    java_code = re.sub(singleline_pattern, \"\", java_code, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    java_code = re.sub(multiline_pattern, \"\", java_code, re.MULTILINE)\n",
    "\n",
    "    # Remove Unit Tests:\n",
    "    java_code = re.sub(test_pattern, \"\", java_code, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove empty white lines:\n",
    "    java_code = re.sub(whiteline_pattern, \"\", java_code)\n",
    "\n",
    "    return java_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fbd9a-5bff-41bf-bd48-b2df0b922d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Set up the codebase chunks and embed them\n",
    "    - Clone the repo down (commented out right now so I don't have to wait for a clone each time it fails)\n",
    "    - Extract & Clean (using above remove_comments() function) the java files\n",
    "    - Chunk the java files into 1600 token chunks\n",
    "    - Store the chunks in memory\n",
    "    - Send the chunks off to openAI's embedding API endpoint (using text-embedding-ada-002)\n",
    "    - Store the embedded text as a Pandas Dataframe\n",
    "    - Print the dataframe (just to be sure)\n",
    "\"\"\"\n",
    "\n",
    "repo_dir = \"fineract\"  # Use the cleaned repo\n",
    "# Clone the GitHub repository -- should really only need to do this once\n",
    "# repo_url = \"https://github.com/apache/fineract\"\n",
    "# subprocess.run([\"git\", \"clone\", \"-b\", \"1.8.4\", repo_url, repo_dir]) # Grab the 1.8.4 fineract git branch just because it's stable\n",
    "\n",
    "# Set up the tokenizer\n",
    "# tokenizer = BBPET()  # Byte\n",
    "tokenizer = SPBPET()   # Sentence\n",
    "\n",
    "\n",
    "java_files = []\n",
    "for root, dirs, files in os.walk(repo_dir):\n",
    "    if dirs == \"test\":\n",
    "        os.rmdir(dirs)\n",
    "    for file in files:\n",
    "        if file.endswith(\".java\"):\n",
    "            java_files.append(os.path.join(root, file))\n",
    "\n",
    "# Process the Java files and break them into 1600 token chunks\n",
    "new_dir = \"fineract-java\"\n",
    "output_dir = \"java_files_chunks_sentence\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "for file in java_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        java_code = f.read()\n",
    "\n",
    "    # Use the remove_comments function to redefine java_code as sans-comments java code:\n",
    "    java_code = remove_comments(java_code)\n",
    "\n",
    "    new_filename = f\"cleaned-{os.path.basename(file)}\"\n",
    "    new_filepath = os.path.join(new_dir, new_filename)\n",
    "\n",
    "    with open(new_filepath, \"w\") as f:\n",
    "        f.write(java_code)\n",
    "\n",
    "new_java_files = []\n",
    "for root, dirs, files in os.walk(new_dir):\n",
    "    if dirs == \"test\":\n",
    "        os.rmdir(dirs)\n",
    "    for file in files:\n",
    "        if file.endswith(\".java\"):\n",
    "            new_java_files.append(os.path.join(root, file))\n",
    "\n",
    "# Train the tokenizer on the Java files:\n",
    "# tokenizer_trainer = BpeTrainer(vocab_size=1600, min_frequency=2)\n",
    "tokenizer.train(new_java_files)\n",
    "\n",
    "output_content = []\n",
    "for file in new_java_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        new_java_code = f.read()\n",
    "\n",
    "    encoding = tokenizer.encode(new_java_code)\n",
    "    tokens = encoding.tokens\n",
    "    ids = encoding.ids\n",
    "    chunk_size = 1600\n",
    "    num_chunks = (len(tokens) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_ids = ids[start:end]\n",
    "        chunk_code = tokenizer.decode(chunk_ids)\n",
    "\n",
    "        chunk_filename = os.path.basename(file) + f\".chunk{i+1}.java\"\n",
    "        chunk_filepath = os.path.join(output_dir, chunk_filename)\n",
    "\n",
    "        output_content.append(chunk_code)\n",
    "        # If you want to visualize the tokenized java chunks, uncomment the following two lines:\n",
    "        # with open(chunk_filepath, \"w\") as f:\n",
    "        #     f.write(chunk_code)\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Number of token chunks to send at a time\n",
    "# OpenAI's example specifies 1000 but I've had greater success with 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(output_content), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = output_content[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": output_content, \"embedding\": embeddings})\n",
    "\n",
    "# Print out the Dataframe, just to be sure:\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13785bd-f7e3-4c2c-a8ce-52aae71e9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory so that py is happy\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# save document chunks and embeddings\n",
    "SAVE_PATH = \"data/fineract.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de4d4b-a0e7-464f-93ff-330e9decfb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_path = SAVE_PATH\n",
    "\n",
    "df = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ced70-3b73-429e-8c3b-029032d23076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357093c1-9cfe-4f2b-8190-e68e3b541803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Setup the vector search function that scrapes through your dataframe and/or CSV file to find related code based on your prompt\n",
    "\"\"\"\n",
    "\n",
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f5c52-ed3d-44c0-ad13-9f8c9c032740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"[Optional] Test the vector scraping from the embedded text\n",
    "    Don't actually need to run this, but it's good to understand\n",
    "    what it's doing under the hood\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Write a new unit test for CodesApiResource {\"\n",
    "\n",
    "strings, relatednesses = strings_ranked_by_relatedness(prompt, df, top_n=3)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf27de-37c2-4c08-82e9-2b31011c1f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Define the functions for how to reach out to ChatGPT\n",
    "    use by calling ask(<prompt>)\n",
    "    Can optionally get the hidden context printed by typing\n",
    "\n",
    "        ask(<prompt>, print_message=True)\n",
    "\"\"\"\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below codeset from the fineract java web application to answer the subsequent question. If the answer cannot be found from the code sample, write \"I could not find an answer.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nFineract Application codebase selection:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about the Fineract Web Application.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c019aee-6d0d-4964-830b-7198e0965fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Call out to ChatGPT\n",
    "\n",
    "Syntax:\n",
    "    \n",
    "    ask(\"<prompt>\", print_message=<True/False>)\n",
    "\n",
    "print_message defaults to false; if set to True it displays the hidden context along with the answer provided by ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "# Ask a question while hiding the context:\n",
    "ask(prompt,print_message=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38fa32-7581-4bfb-a692-2ce344abdadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
